{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endpoint Feature Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohere.command-latest\n",
      "cohere.command-a-03-2025\n",
      "cohere.command-r-plus-08-2024\n",
      "cohere.command-r-08-2024\n",
      "meta.llama-latest\n",
      "meta.llama-4-maverick-17b-128e-instruct-fp8\n",
      "meta.llama-4-scout-17b-16e-instruct\n",
      "meta.llama-3.3-70b-instruct\n",
      "meta.llama-3.2-11b-vision-instruct\n",
      "meta.llama-3.2-90b-vision-instruct\n",
      "my-dedicated-model-name\n",
      "my-datascience-model-name\n",
      "ODSC-Mistral-7B-Instruct\n",
      "ODSC-DeepSeek-R1-Distill-Qwen-7B\n",
      "ODSC-sql-coder-graph\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = \"ocigenerativeai\",\n",
    "    base_url = \"http://127.0.0.1:8088/api/v1/\",\n",
    "    )\n",
    "models = client.models.list()\n",
    "\n",
    "# Test chat completions\n",
    "for model in models:\n",
    "    print(model.id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test chat completions with non-streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_models = [\n",
    "    \"cohere.command-a-03-2025\",\n",
    "    \"cohere.command-r-plus-08-2024\",\n",
    "    \"cohere.command-r-08-2024\",\n",
    "    \"meta.llama-3.3-70b-instruct\",\n",
    "    \"meta.llama-3.2-90b-vision-instruct\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello!\n",
      "******************** Model: cohere.command-a-03-2025 ******************** \n",
      "Assistant:Hello! How can I assist you today? Whether you\n",
      "******************** Model: cohere.command-r-plus-08-2024 ******************** \n",
      "Assistant:Hello! How can I help you today?\n",
      "******************** Model: cohere.command-r-08-2024 ******************** \n",
      "Assistant:Hello! How can I assist you today?\n",
      "******************** Model: meta.llama-3.3-70b-instruct ******************** \n",
      "Assistant:Hello. It's nice to meet you. Is there something\n",
      "******************** Model: meta.llama-3.2-90b-vision-instruct ******************** \n",
      "Assistant:Hello. It's nice to meet you. Is there something\n"
     ]
    }
   ],
   "source": [
    "# Test chat completions with non-streaming response\n",
    "message = \"Hello!\"\n",
    "print(\"User:\", message)\n",
    "\n",
    "for model_name in test_models:\n",
    "    print(\"*\"*20,\"Model:\",model_name,\"*\"*20,\"\\nAssistant:\", end='')\n",
    "    completion = client.chat.completions.create(\n",
    "        model = model_name,\n",
    "        messages = [{\"role\": \"user\", \"content\": message}],\n",
    "        max_tokens=12,\n",
    "        )\n",
    "    print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test chat completions with streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello!\n",
      "\n",
      " ******************** Model: cohere.command-a-03-2025 ******************** \n",
      "Assistant:Hello! How can I assist you today? Whether you\n",
      " ******************** Model: cohere.command-r-plus-08-2024 ******************** \n",
      "Assistant:Hello! How can I help you today?\n",
      " ******************** Model: cohere.command-r-08-2024 ******************** \n",
      "Assistant:Hello! How can I assist you today?\n",
      " ******************** Model: meta.llama-3.3-70b-instruct ******************** \n",
      "Assistant:Hello. It's nice to meet you. Is there\n",
      " ******************** Model: meta.llama-3.2-90b-vision-instruct ******************** \n",
      "Assistant:Hello. It's nice to meet you. Is there"
     ]
    }
   ],
   "source": [
    "message = \"Hello!\"\n",
    "print(\"User:\", message)\n",
    "\n",
    "for model in test_models:    \n",
    "    print('\\n',\"*\"*20,\"Model:\",model,\"*\"*20,\"\\nAssistant:\", end='')\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{'role': 'user', 'content': message}],\n",
    "        max_tokens=12,\n",
    "        stream=True  # this time, we set stream=True\n",
    "    )\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content,end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test multi-modal with image input response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAeAB4AAD/4QAiRXhpZgAATU0AKgAAAAgAAQESAAMAAAABAAEAAAAAAAD/2\n",
      "\n",
      " ******************** Model: meta.llama-3.2-90b-vision-instruct ********************\n",
      "Assistant:\n",
      " This image showcases the iconic Superman logo, a stylized shield with a distinctive red and yellow color scheme. The logo features a red shield shape with a yellow \"S\" inside, which is a nod to Superman's Kryptonian heritage. The shield is outlined in black, adding depth and contrast to the design.\n",
      "\n",
      "The background of the image is a solid blue color, which provides a clean and simple backdrop for the logo. The overall aesthetic of the image is bold and eye-catching, making it instantly recognizable as the symbol of the Man of Steel.\n",
      "\n",
      "In terms of specific details, the image appears to\n"
     ]
    }
   ],
   "source": [
    "test_models = [\"meta.llama-3.2-90b-vision-instruct\"]\n",
    "\n",
    "import base64\n",
    "import mimetypes\n",
    "\n",
    "image_path = \"./test/image.jpg\"\n",
    "mime_type, _ = mimetypes.guess_type(image_path)\n",
    "with open(image_path, \"rb\") as image_file:\n",
    "    base64_str = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "url = f\"data:{mime_type};base64,{base64_str}\"\n",
    "print(url[:100])\n",
    "\n",
    "content = [\n",
    "    {\"type\": \"text\",\"text\": \"describe this image?\"},\n",
    "\t{\"type\": \"image_url\",\"image_url\": {\"url\": url}}\n",
    "    ]\n",
    "\n",
    "for model in test_models:\n",
    "    print('\\n',\"*\"*20,\"Model:\",model,\"*\"*20)\n",
    "    completion = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [{\"role\": \"user\", \"content\": content}],\n",
    "        max_tokens=120,\n",
    "        )\n",
    "    print(\"Assistant:\\n\", completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(latitude, longitude):\n",
    "    import requests\n",
    "    response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\")\n",
    "    data = response.json()\n",
    "    return data['current']['temperature_2m']\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current temperature for provided coordinates in celsius.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"latitude\": {\"type\": \"number\"},\n",
    "                \"longitude\": {\"type\": \"number\"}\n",
    "            },\n",
    "            \"required\": [\"latitude\", \"longitude\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cohere and Llama model without streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function call output for model -- cohere.command-a-03-2025\n",
      "[ChatCompletionMessageToolCall(id='eyJuYW1lIjogImdldF93ZWF0aGVyIiwgImFyZ3VtZW50cyI6ICJ7XCJsYXRpdHVkZVwiOiA0OC44NTY2LCBcImxvbmdpdHVkZVwiOiAyLjM1MjJ9In0=', function=Function(arguments='{\"latitude\": 48.8566, \"longitude\": 2.3522}', name='get_weather'), type='function', index=0), ChatCompletionMessageToolCall(id='eyJuYW1lIjogImdldF93ZWF0aGVyIiwgImFyZ3VtZW50cyI6ICJ7XCJsYXRpdHVkZVwiOiA0MC43MTI4LCBcImxvbmdpdHVkZVwiOiAtNzQuMDA2fSJ9', function=Function(arguments='{\"latitude\": 40.7128, \"longitude\": -74.006}', name='get_weather'), type='function', index=1)] \n",
      "\n",
      "Function call output for model -- meta.llama-3.3-70b-instruct\n",
      "[ChatCompletionMessageToolCall(id='chatcmpl-tool-4c002eb5bcfb4353bda8f8ac0502ef86', function=Function(arguments='{\"latitude\": \"48.8566\", \"longitude\": \"2.3522\"}', name='get_weather'), type='function', index=0)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Cohere and Llama model without streaming\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is the weather like in Paris and New York today?\"}]\n",
    "\n",
    "for model in [\"cohere.command-a-03-2025\",\n",
    "              \"meta.llama-3.3-70b-instruct\"]:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        stream=False\n",
    "        )\n",
    "    print(f\"Function call output for model -- {model}\")\n",
    "    print(completion.choices[0].message.tool_calls,\"\\n\")\n",
    "    \n",
    "    if model.startswith(\"cohere\"):\n",
    "        new_message = completion.choices[0].message\n",
    "        tool_calls = completion.choices[0].message.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cohere and Llama model with streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Function call output for model -- cohere.command-a-03-2025\n",
      "I will use the get_weather tool to find the current temperature in Paris and New York.I will use the get_weather tool to find the current temperature in Paris and New York.\n",
      "\n",
      "[ChoiceDeltaToolCall(index=0, id='eyJuYW1lIjogImdldF93ZWF0aGVyIiwgImFyZ3VtZW50cyI6ICJ7XCJsYXRpdHVkZVwiOiA0OC44NTY2LCBcImxvbmdpdHVkZVwiOiAyLjM1MjJ9In0=', function=ChoiceDeltaToolCallFunction(arguments='{\"latitude\": 48.8566, \"longitude\": 2.3522}', name='get_weather'), type='function'), ChoiceDeltaToolCall(index=1, id='eyJuYW1lIjogImdldF93ZWF0aGVyIiwgImFyZ3VtZW50cyI6ICJ7XCJsYXRpdHVkZVwiOiA0MC43MTI4LCBcImxvbmdpdHVkZVwiOiAtNzQuMDA2fSJ9', function=ChoiceDeltaToolCallFunction(arguments='{\"latitude\": 40.7128, \"longitude\": -74.006}', name='get_weather'), type='function')]\n",
      "\n",
      "Function call output for model -- meta.llama-3.3-70b-instruct\n",
      "[ChoiceDeltaToolCall(index=0, id='chatcmpl-tool-f5158515118b4b4bb59e69ff040e321d', function=ChoiceDeltaToolCallFunction(arguments=None, name='get_weather'), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"latitude\": \"', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='48', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='.', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='856', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='7\"', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=', \"longitude\": \"', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='2', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='.', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='351', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='0\"}', name=None), type='function')]\n",
      "None"
     ]
    }
   ],
   "source": [
    "#### Cohere and Llama model with streaming\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is the weather like in Paris and New York today?\"}]\n",
    "\n",
    "for model in [\"cohere.command-a-03-2025\",\n",
    "              \"meta.llama-3.3-70b-instruct\"]:\n",
    "    response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            stream=True  # this time, we set stream=True\n",
    "        )\n",
    "    print(f\"\\nFunction call output for model -- {model}\")\n",
    "    for chunk in response:    \n",
    "        if not chunk.choices[0].delta.tool_calls:\n",
    "            print(chunk.choices[0].delta.content,end='')\n",
    "        else:\n",
    "            if model.startswith(\"cohere\"):\n",
    "                print(\"\\n\")\n",
    "            print(chunk.choices[0].delta.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'What is the weather like in Paris and New York today?'}, ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='eyJuYW1lIjogImdldF93ZWF0aGVyIiwgImFyZ3VtZW50cyI6ICJ7XCJsYXRpdHVkZVwiOiA0OC44NTY2LCBcImxvbmdpdHVkZVwiOiAyLjM1MjJ9In0=', function=Function(arguments='{\"latitude\": 48.8566, \"longitude\": 2.3522}', name='get_weather'), type='function', index=0), ChatCompletionMessageToolCall(id='eyJuYW1lIjogImdldF93ZWF0aGVyIiwgImFyZ3VtZW50cyI6ICJ7XCJsYXRpdHVkZVwiOiA0MC43MTI4LCBcImxvbmdpdHVkZVwiOiAtNzQuMDA2fSJ9', function=Function(arguments='{\"latitude\": 40.7128, \"longitude\": -74.006}', name='get_weather'), type='function', index=1)])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages.append(new_message)\n",
    "for each in messages:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.9\n",
      "13.3\n",
      "{'role': 'user', 'content': 'What is the weather like in Paris and New York today?'}\n",
      "{'role': 'tool', 'tool_call_id': 'eyJuYW1lIjogImdldF93ZWF0aGVyIiwgImFyZ3VtZW50cyI6ICJ7XCJsYXRpdHVkZVwiOiA0OC44NTY2LCBcImxvbmdpdHVkZVwiOiAyLjM1MjJ9In0=', 'content': '14.9'}\n",
      "{'role': 'tool', 'tool_call_id': 'eyJuYW1lIjogImdldF93ZWF0aGVyIiwgImFyZ3VtZW50cyI6ICJ7XCJsYXRpdHVkZVwiOiA0MC43MTI4LCBcImxvbmdpdHVkZVwiOiAtNzQuMDA2fSJ9', 'content': '13.3'}\n"
     ]
    }
   ],
   "source": [
    "# use tool to get weather\n",
    "import json\n",
    "for tool_call in tool_calls:\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "    result = get_weather(args[\"latitude\"], args[\"longitude\"])\n",
    "    print(result)\n",
    "    # append result message\n",
    "    messages.append({                               \n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": tool_call.id,\n",
    "        \"content\": str(result)\n",
    "    })\n",
    "\n",
    "for each in messages:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output for model -- cohere.command-a-03-2025\n",
      "I'm unable to provide the current weather in Paris and New York as I don't have access to real-time information. However, I can suggest checking a reliable weather website or using a weather application on your device for the most up-to-date information.\n",
      "\n",
      "If you'd like, I can provide some general information about the climate in these cities. Paris typically has a temperate climate with mild winters and warm summers, while New York experiences a humid subtropical climate with hot summers and cold winters.\n",
      "\n",
      "Please note that these are general descriptions and may not reflect the current weather conditions. For accurate and current weather information, please consult a reliable source.\n",
      "\n",
      "Output for model -- meta.llama-3.3-70b-instruct\n",
      "The current weather in Paris is mostly cloudy with a high of 14.9°C and a low of 13.3°C. The current weather in New York is partly cloudy with a high of 14.9°C and a low of 13.3°C.\n"
     ]
    }
   ],
   "source": [
    "# Generate answer based on tool results without stream\n",
    "\n",
    "for model in [\"cohere.command-a-03-2025\",\n",
    "              \"meta.llama-3.3-70b-instruct\"]:\n",
    "    completion_2 = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=False\n",
    "    )\n",
    "    print(f\"\\nOutput for model -- {model}\")\n",
    "    print(completion_2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output for model -- cohere.command-a-03-2025\n",
      "I'm unable to provide the current weather in Paris and New York as I don't have access to real-time information. However, I can suggest checking a reliable weather website or using a weather application on your device for the most up-to-date information.\n",
      "\n",
      "If you'd like, I can provide some general information about the climate in these cities. Paris typically has a temperate climate with mild winters and warm summers, while New York experiences a humid subtropical climate with hot summers and cold winters.\n",
      "\n",
      "Please note that weather conditions can change rapidly, so it's always best to consult a reliable source for the most accurate and current information.\n",
      "Output for model -- meta.llama-3.3-70b-instruct\n",
      "I'm not able to tell you the weather in Paris and New York today. I'm a large language model, I don't have access to real-time information about the current weather in specific locations. I can suggest some ways for you to find out the current weather in Paris and New York, such as checking online weather websites like AccuWeather or Weather.com, or using a mobile app like Dark Sky or Weather Underground. Additionally, you can also check the official websites of the national meteorological services for France (Météo-France) and the United States (National Weather Service) for the latest weather forecasts and updates."
     ]
    }
   ],
   "source": [
    "# Generate answer based on tool results with stream\n",
    "for model in [\"cohere.command-a-03-2025\",\n",
    "              \"meta.llama-3.3-70b-instruct\"]:\n",
    "    completion_2 = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "        )\n",
    "    print(f\"\\nOutput for model -- {model}\")\n",
    "    for chunk in completion_2:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content,end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: cohere.embed-english-light-v3.0\n",
      "0 : [0.049591064453125, 0.03173828125, - ...... 0824737548828125, 0.021087646484375]\n",
      "1 : [-0.06787109375, -0.05828857421875,  ...... .0199127197265625, 0.04034423828125]\n",
      "****************************************************************************************************\n",
      "model: cohere.embed-english-v3.0\n",
      "0 : [-0.015960693359375, 0.0127182006835 ...... -0.0203857421875, 0.020050048828125]\n",
      "1 : [-0.01410675048828125, -0.0110168457 ...... 021575927734375, 0.0208587646484375]\n",
      "****************************************************************************************************\n",
      "model: cohere.embed-multilingual-light-v3.0\n",
      "0 : [-0.018890380859375, 0.0327453613281 ...... 398193359375, 0.0008997917175292969]\n",
      "1 : [0.038238525390625, 0.01398468017578 ......  0.08721923828125, 0.02984619140625]\n",
      "****************************************************************************************************\n",
      "model: cohere.embed-multilingual-v3.0\n",
      "0 : [-0.00600433349609375, 0.03067016601 ...... 4055023193359375, -0.01837158203125]\n",
      "1 : [-0.008331298828125, 0.0429382324218 ...... 301040649414062, -0.045501708984375]\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "embd_model=[ 'cohere.embed-english-light-v3.0',\n",
    "            'cohere.embed-english-v3.0',\n",
    "            'cohere.embed-multilingual-light-v3.0',\n",
    "            'cohere.embed-multilingual-v3.0']\n",
    "input = [\"hello!\",\"你好！\"]\n",
    "for model in embd_model:\n",
    "    print(\"model:\",model)\n",
    "    response = client.embeddings.create(input = input, model=model).data\n",
    "    for each in response:\n",
    "        print(each.index,':',str(each.embedding)[:36],'......',str(each.embedding)[-36:])\n",
    "    print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
