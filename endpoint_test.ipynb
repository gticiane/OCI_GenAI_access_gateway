{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endpoint Feature Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohere.command-latest\n",
      "cohere.command-a-03-2025\n",
      "cohere.command-r-plus-08-2024\n",
      "cohere.command-r-08-2024\n",
      "meta.llama-latest\n",
      "meta.llama-4-maverick-17b-128e-instruct-fp8\n",
      "meta.llama-4-scout-17b-16e-instruct\n",
      "meta.llama-3.3-70b-instruct\n",
      "meta.llama-3.2-11b-vision-instruct\n",
      "meta.llama-3.2-90b-vision-instruct\n",
      "openai.gpt-4.1\n",
      "openai.gpt-4.1-2025-04-14\n",
      "openai.gpt-4.1-mini\n",
      "openai.gpt-4.1-mini-2025-04-14\n",
      "openai.gpt-4.1-nano\n",
      "openai.gpt-4.1-nano-2025-04-14\n",
      "openai.gpt-4o\n",
      "openai.gpt-4o-2024-08-06\n",
      "openai.gpt-4o-2024-11-20\n",
      "openai.gpt-4o-mini\n",
      "openai.gpt-4o-mini-2024-07-18\n",
      "openai.gpt-4o-mini-search-preview\n",
      "openai.gpt-4o-mini-search-preview-2025-03-11\n",
      "openai.gpt-4o-search-preview\n",
      "openai.gpt-4o-search-preview-2025-03-11\n",
      "openai.o1\n",
      "openai.o1-2024-12-17\n",
      "openai.o3\n",
      "openai.o3-2025-04-16\n",
      "openai.o3-mini\n",
      "openai.o3-mini-2025-01-31\n",
      "openai.o4-mini\n",
      "my-dedicated-model-name\n",
      "my-datascience-model-name\n",
      "ODSC-Mistral-7B-Instruct\n",
      "ODSC-DeepSeek-R1-Distill-Qwen-7B\n",
      "ODSC-sql-coder-graph\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = \"ocigenerativeai\",\n",
    "    base_url = \"http://127.0.0.1:8088/v1/\",\n",
    "    )\n",
    "models = client.models.list()\n",
    "\n",
    "# Test chat completions\n",
    "for model in models:\n",
    "    print(model.id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test chat completions with non-streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_models = [\n",
    "    \"openai.gpt-4o\",\n",
    "    \"cohere.command-latest\",\n",
    "    \"meta.llama-latest\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai.gpt-4o  :  Hello! How can I assist you today?\n",
      "cohere.command-latest  :  Hello! How can I assist you today?\n",
      "meta.llama-latest  :  Hello! It's nice to meet you. Is there something I\n"
     ]
    }
   ],
   "source": [
    "# Test chat completions with non-streaming response\n",
    "\n",
    "for model_name in test_models:\n",
    "    completion = client.chat.completions.create(\n",
    "        model = model_name,\n",
    "        messages = [{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "        max_tokens=12,\n",
    "        )\n",
    "    print(model_name, \" : \", completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test chat completions with streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " openai.gpt-4o  : Hello! How can I assist you today?\n",
      " cohere.command-latest  : Hello! How can I assist you today?\n",
      " meta.llama-latest  : Hello! How are you today? Is there something I"
     ]
    }
   ],
   "source": [
    "for model in test_models:    \n",
    "    print('\\n',model,\" : \",end='')\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{'role': 'user', 'content': \"hello\"}],\n",
    "        max_tokens=12,\n",
    "        stream=True  # this time, we set stream=True\n",
    "    )\n",
    "    for chunk in response:        \n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content,end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test multi-modal with image input response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAeAB4AAD/4QAiRXhpZgAATU0AKgAAAAgAAQESAAMAAAABAAEAAAAAAAD/2\n",
      "\n",
      " ******************** Model: openai.gpt-4o ********************\n",
      "Assistant:\n",
      " This image features a stylized, iconic logo in the shape of a red and yellow \"S\" shield. The background is blue, and the shield design is typically associated with a well-known superhero character.\n"
     ]
    }
   ],
   "source": [
    "test_models = [\"openai.gpt-4o\"]\n",
    "\n",
    "import base64\n",
    "import mimetypes\n",
    "\n",
    "image_path = \"./test/image.jpg\"\n",
    "mime_type, _ = mimetypes.guess_type(image_path)\n",
    "with open(image_path, \"rb\") as image_file:\n",
    "    base64_str = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "url = f\"data:{mime_type};base64,{base64_str}\"\n",
    "print(url[:100])\n",
    "\n",
    "content = [\n",
    "    {\"type\": \"text\",\"text\": \"describe this image?\"},\n",
    "\t{\"type\": \"image_url\",\"image_url\": {\"url\": url}}\n",
    "    ]\n",
    "\n",
    "for model in test_models:\n",
    "    print('\\n',\"*\"*20,\"Model:\",model,\"*\"*20)\n",
    "    completion = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [{\"role\": \"user\", \"content\": content}],\n",
    "        max_tokens=120,\n",
    "        )\n",
    "    print(\"Assistant:\\n\", completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(latitude, longitude):\n",
    "    import requests\n",
    "    response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\")\n",
    "    data = response.json()\n",
    "    return data['current']['temperature_2m']\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current temperature for provided coordinates in celsius.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"latitude\": {\"type\": \"number\"},\n",
    "                \"longitude\": {\"type\": \"number\"}\n",
    "            },\n",
    "            \"required\": [\"latitude\", \"longitude\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}]\n",
    "\n",
    "#model = \"cohere.command-a-03-2025\"\n",
    "#model = \"meta.llama-3.3-70b-instruct\"\n",
    "model = \"openai.gpt-4o\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response without streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function call output for model -- openai.gpt-4o\n",
      "[ChatCompletionMessageToolCall(id='call_x2VjsKQHoV8t7J5F6pZuSLBY', function=Function(arguments='{\"latitude\":40.7128,\"longitude\":-74.006}', name='get_weather'), type='function', index=0)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Cohere and Llama model without streaming\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Use tool to answer: What is the weather like in New York today?\"}]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    stream=False\n",
    "    )\n",
    "print(f\"Function call output for model -- {model}\")\n",
    "print(completion.choices[0].message.tool_calls,\"\\n\")\n",
    "\n",
    "new_message = completion.choices[0].message\n",
    "tool_calls = completion.choices[0].message.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response with streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai.gpt-4o :\n",
      "[ChoiceDeltaToolCall(index=0, id='call_BWZrMiww8u2hk22em2mdcCsv', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='latitude', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='40', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='.', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='712', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='8', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=',\"', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='longitude', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='-', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='74', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='.', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='006', name=None), type='function')]\n",
      "[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='}', name=None), type='function')]\n",
      "None"
     ]
    }
   ],
   "source": [
    "#### Cohere and Llama model with streaming\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Use tool to answer: What is the weather like in New York today?\"}]\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        stream=True  # this time, we set stream=True\n",
    "    )\n",
    "print(model,\":\")\n",
    "for chunk in response:    \n",
    "    if not chunk.choices[0].delta.tool_calls:\n",
    "        print(chunk.choices[0].delta.content,end='')\n",
    "    else:\n",
    "        if model.startswith(\"cohere\"):\n",
    "            print(\"\\n\")\n",
    "        print(chunk.choices[0].delta.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'Use tool to answer: What is the weather like in New York today?'}\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_x2VjsKQHoV8t7J5F6pZuSLBY', function=Function(arguments='{\"latitude\":40.7128,\"longitude\":-74.006}', name='get_weather'), type='function', index=0)])\n"
     ]
    }
   ],
   "source": [
    "#### Add messages to the conversation\n",
    "\n",
    "messages.append(new_message)\n",
    "for each in messages:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.5\n",
      "{'role': 'user', 'content': 'Use tool to answer: What is the weather like in New York today?'}\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_x2VjsKQHoV8t7J5F6pZuSLBY', function=Function(arguments='{\"latitude\":40.7128,\"longitude\":-74.006}', name='get_weather'), type='function', index=0)])\n",
      "{'role': 'tool', 'tool_call_id': 'call_x2VjsKQHoV8t7J5F6pZuSLBY', 'content': '20.5'}\n"
     ]
    }
   ],
   "source": [
    "# use tool to get weather\n",
    "import json\n",
    "for tool_call in tool_calls:\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "    result = get_weather(args[\"latitude\"], args[\"longitude\"])\n",
    "    print(result)\n",
    "    # append result message\n",
    "    messages.append({                               \n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": tool_call.id,\n",
    "        \"content\": str(result)\n",
    "    })\n",
    "\n",
    "for each in messages:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output for model -- openai.gpt-4o\n",
      "The current temperature in New York is 20.5°C. Unfortunately, I don't have detailed information about other weather conditions at this time.\n"
     ]
    }
   ],
   "source": [
    "# Generate answer based on tool results without stream\n",
    "\n",
    "completion_2 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    stream=False\n",
    ")\n",
    "print(f\"\\nOutput for model -- {model}\")\n",
    "print(completion_2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output for model -- openai.gpt-4o\n",
      "The weather in New York today is 20.5°C."
     ]
    }
   ],
   "source": [
    "# Generate answer based on tool results with stream\n",
    "\n",
    "completion_2 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    stream=True\n",
    "    )\n",
    "print(f\"\\nOutput for model -- {model}\")\n",
    "for chunk in completion_2:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content,end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_model=[ 'cohere.embed-english-light-v3.0',\n",
    "            'cohere.embed-english-v3.0',\n",
    "            'cohere.embed-multilingual-light-v3.0',\n",
    "            'cohere.embed-multilingual-v3.0']\n",
    "input = [\"hello!\",\"你好！\"]\n",
    "for model in embd_model:\n",
    "    print(\"model:\",model)\n",
    "    response = client.embeddings.create(input = input, model=model).data\n",
    "    for each in response:\n",
    "        print(each.index,':',str(each.embedding)[:36],'......',str(each.embedding)[-36:])\n",
    "    print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
