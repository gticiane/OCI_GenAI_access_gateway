{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endpoint test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohere.command-r-plus-08-2024\n",
      "cohere.command-r-08-2024\n",
      "cohere.command-r-plus\n",
      "cohere.command-r-16k\n",
      "meta.llama-3.3-70b-instruct\n",
      "meta.llama-3.2-11b-vision-instruct\n",
      "meta.llama-3.2-90b-vision-instruct\n",
      "my-dedicated-model-name\n",
      "my-datascience-model-name\n",
      "ODSC-Mistral-7B-Instruct\n",
      "ODSC-DeepSeek-R1-Distill-Qwen-7B\n",
      "ODSC-sql-coder-graph\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = \"ocigenerativeai\",\n",
    "    base_url = \"http://127.0.0.1:8088/api/v1/\",\n",
    "    )\n",
    "models = client.models.list()\n",
    "\n",
    "# Test chat completions\n",
    "for model in models:\n",
    "    print(model.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_models = [\n",
    "    \"cohere.command-r-plus-08-2024\",\n",
    "    \"cohere.command-r-08-2024\",\n",
    "    \"meta.llama-3.3-70b-instruct\",\n",
    "    \"meta.llama-3.2-90b-vision-instruct\",\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test chat completions with non-streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello!\n",
      "******************** Model: cohere.command-r-plus-08-2024 ******************** \n",
      "Assistant:Hello! How can I help you today?\n",
      "******************** Model: cohere.command-r-08-2024 ******************** \n",
      "Assistant:Hello! How can I assist you today?\n",
      "******************** Model: meta.llama-3.3-70b-instruct ******************** \n",
      "Assistant:Hello. It's nice to meet you. Is there something\n",
      "******************** Model: meta.llama-3.2-90b-vision-instruct ******************** \n",
      "Assistant:Hello. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Test chat completions with non-streaming response\n",
    "message = \"Hello!\"\n",
    "print(\"User:\", message)\n",
    "\n",
    "for model_name in test_models:\n",
    "    print(\"*\"*20,\"Model:\",model_name,\"*\"*20,\"\\nAssistant:\", end='')\n",
    "    completion = client.chat.completions.create(\n",
    "        model = model_name,\n",
    "        messages = [{\"role\": \"user\", \"content\": message}],\n",
    "        max_tokens=12,\n",
    "        )\n",
    "    print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test chat completions with streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello!\n",
      "\n",
      " ******************** Model: cohere.command-r-plus-08-2024 ******************** \n",
      "Assistant:Hello! How can I help you today?\n",
      " ******************** Model: cohere.command-r-08-2024 ******************** \n",
      "Assistant:Hello! How can I assist you today?\n",
      " ******************** Model: meta.llama-3.3-70b-instruct ******************** \n",
      "Assistant:Hello. How can I help you today?\n",
      " ******************** Model: meta.llama-3.2-90b-vision-instruct ******************** \n",
      "Assistant:Hello. How can I help you today."
     ]
    }
   ],
   "source": [
    "message = \"Hello!\"\n",
    "print(\"User:\", message)\n",
    "\n",
    "for model in test_models:    \n",
    "    print('\\n',\"*\"*20,\"Model:\",model,\"*\"*20,\"\\nAssistant:\", end='')\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{'role': 'user', 'content': message}],\n",
    "        max_tokens=12,\n",
    "        stream=True  # this time, we set stream=True\n",
    "    )\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content,end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test multi-modal with image input response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAeAB4AAD/4QAiRXhpZgAATU0AKgAAAAgAAQESAAMAAAABAAEAAAAAAAD/2\n",
      "\n",
      " ******************** Model: meta.llama-3.2-90b-vision-instruct ********************\n",
      "Model: meta.llama-3.2-90b-vision-instruct\n",
      "Assistant:\n",
      " The image features the iconic Superman logo, a red shield with a yellow \"S\" in the center and a black outline. The shield is diamond-shaped, with two triangular shapes on either side of the \"S,\" resembling wings. The background of the image is a solid blue color.\n",
      "\n",
      "**Key Elements:**\n",
      "\n",
      "* **Logo:** A red shield with a yellow \"S\" in the center and a black outline.\n",
      "* **Shield Shape:** Diamond-shaped with two triangular shapes on either side of the \"S,\" resembling wings.\n",
      "* **Background:** A solid blue color.\n",
      "\n",
      "**Overall Impression:**\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test multi-modal with image input response\n",
    "import base64\n",
    "import mimetypes\n",
    "\n",
    "image_path = \"image.jpg\"\n",
    "mime_type, _ = mimetypes.guess_type(image_path)\n",
    "with open(image_path, \"rb\") as image_file:\n",
    "    base64_str = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "url = f\"data:{mime_type};base64,{base64_str}\"\n",
    "print(url[:100])\n",
    "\n",
    "content = [\n",
    "    {\"type\": \"text\",\"text\": \"describe this image?\"},\n",
    "\t{\"type\": \"image_url\",\"image_url\": {\"url\": url}}\n",
    "    ]\n",
    "\n",
    "test_models = [\"meta.llama-3.2-90b-vision-instruct\"]\n",
    "\n",
    "for model in test_models:\n",
    "    print('\\n',\"*\"*20,\"Model:\",model,\"*\"*20)\n",
    "    print(\"Model:\", model)\n",
    "    completion = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [{\"role\": \"user\", \"content\": content}],\n",
    "        max_tokens=120,\n",
    "        )\n",
    "    print(\"Assistant:\\n\", completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessageToolCall(id='eyJuYW1lIjogImdldF93ZWF0aGVyIiwgImFyZ3VtZW50cyI6ICJ7XCJsYXRpdHVkZVwiOiA0OC44NTY2LCBcImxvbmdpdHVkZVwiOiAyLjM1MjJ9In0=', function=Function(arguments='{\"latitude\": 48.8566, \"longitude\": 2.3522}', name='get_weather'), type='function', index=0), ChatCompletionMessageToolCall(id='eyJuYW1lIjogImdldF93ZWF0aGVyIiwgImFyZ3VtZW50cyI6ICJ7XCJsYXRpdHVkZVwiOiA0MC43MTI4LCBcImxvbmdpdHVkZVwiOiA3NC4wMDZ9In0=', function=Function(arguments='{\"latitude\": 40.7128, \"longitude\": 74.006}', name='get_weather'), type='function', index=1)]\n"
     ]
    }
   ],
   "source": [
    "def get_weather(latitude, longitude):\n",
    "    import requests\n",
    "    response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\")\n",
    "    data = response.json()\n",
    "    return data['current']['temperature_2m']\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current temperature for provided coordinates in celsius.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"latitude\": {\"type\": \"number\"},\n",
    "                \"longitude\": {\"type\": \"number\"}\n",
    "            },\n",
    "            \"required\": [\"latitude\", \"longitude\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}]\n",
    "model = \"cohere.command-r-plus-08-2024\"\n",
    "# model = \"cohere.command-r-plus-08-2024\"\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is the weather like in Paris and New York today?\"}]\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "messages.append(completion.choices[0].message)\n",
    "\n",
    "print(completion.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.4\n",
      "-4.2\n"
     ]
    }
   ],
   "source": [
    "# use tool to get weather\n",
    "import json\n",
    "for tool_call in completion.choices[0].message.tool_calls:\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "    result = get_weather(args[\"latitude\"], args[\"longitude\"])\n",
    "    print(result)\n",
    "    # append result message\n",
    "    messages.append({                               \n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": tool_call.id,\n",
    "        \"content\": str(result)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's currently 8.4°C in Paris and -4.2°C in New York.\n"
     ]
    }
   ],
   "source": [
    "# Generate answer based on tool results without stream\n",
    "completion_2 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(completion_2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today, the temperature in Paris is 8.4 degrees Celsius, and in New York it is -4.2 degrees Celsius."
     ]
    }
   ],
   "source": [
    "# Generate answer based on tool results with stream\n",
    "completion_2 = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    stream=True\n",
    "    )\n",
    "for chunk in completion_2:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content,end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: cohere.embed-english-light-v3.0\n",
      "0 : [0.049591064453125, 0.03173828125, - ...... 0824737548828125, 0.021087646484375]\n",
      "1 : [-0.06787109375, -0.05828857421875,  ...... .0199127197265625, 0.04034423828125]\n",
      "****************************************************************************************************\n",
      "model: cohere.embed-english-v3.0\n",
      "0 : [-0.015960693359375, 0.0127182006835 ...... -0.0203857421875, 0.020050048828125]\n",
      "1 : [-0.01410675048828125, -0.0110168457 ...... 021575927734375, 0.0208587646484375]\n",
      "****************************************************************************************************\n",
      "model: cohere.embed-multilingual-light-v3.0\n",
      "0 : [-0.018890380859375, 0.0327453613281 ...... 398193359375, 0.0008997917175292969]\n",
      "1 : [0.038238525390625, 0.01398468017578 ......  0.08721923828125, 0.02984619140625]\n",
      "****************************************************************************************************\n",
      "model: cohere.embed-multilingual-v3.0\n",
      "0 : [-0.00600433349609375, 0.03067016601 ...... 4055023193359375, -0.01837158203125]\n",
      "1 : [-0.008331298828125, 0.0429382324218 ...... 301040649414062, -0.045501708984375]\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "embd_model=[ 'cohere.embed-english-light-v3.0',\n",
    "            'cohere.embed-english-v3.0',\n",
    "            'cohere.embed-multilingual-light-v3.0',\n",
    "            'cohere.embed-multilingual-v3.0']\n",
    "input = [\"hello!\",\"你好！\"]\n",
    "for model in embd_model:\n",
    "    print(\"model:\",model)\n",
    "    response = client.embeddings.create(input = input, model=model).data\n",
    "    for each in response:\n",
    "        print(each.index,':',str(each.embedding)[:36],'......',str(each.embedding)[-36:])\n",
    "    print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
